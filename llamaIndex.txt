AI agents can be built with smolagent, llamaindex and langraph....

smolagent use code-like to execute task 

LlamaIndex 
LlamaIndex has three key parts 
1. components
2. Tools
3. Agents 

Components - they are the basic building blocks used in LlamaIndex... They include prompts,models and databases and the components can also help you connect with other tools and libraries 

Tools-they provide specific capabailities... they are the hands of the Llm 

Agents- Agents use the tols to make decisions... they allow to perform the task

workflows - are the step-by-step process that process logic altogether


How to Install LlamaIndex

pip install llama-index-[component-type]-[framework-name]

RAG(Retrieval Augemented Generation)- allows you to find relevant information from data
The QueryEngine allows the agent to what is above

5 stages in a RAG
1. loading- fetching the data from where it lives... maybe a website,pdf,file,another,website,database or api into the your workflows

2. Indexing- a data structure that allows to query the data.. for LLMs they use vector embeddings which is a numerical representation of the data 

3. Storing - stores the index to avoid re-indexing them 

4. Query- self - explanatory

5. Evaluation- a critical step in any flow is checking how effective it is relative to other strategies, or when you make changes. Evaluation provides objective measures of how accurate, faithful and fast your responses to queries are.


Ways to Load data into LlamaIndex

1. SimpleDirectoryReader: A built-in loader for various file types from a local directory.
2. LlamaParse: LlamaParse, LlamaIndexâ€™s official tool for PDF parsing, available as a managed API.
3. LlamaHub: A registry of hundreds of data-loading libraries to ingest data from any source.


SimpleDirectoryReader to load data from a folder.


from llama_index.core import SimpleDirectoryReader
reader = SimpleDirectoryReader(input_dir="path/to/directory")
documents = reader.load_data()

the reader.load_data() breaks it into a document object 

after doing that now we need to break it into smaller pieces called node. A node is a chunmk of text from the original text that is easier to work with. nodes refernce the priginal document 


The IngestionPipeline helps us create these nodes through two key transformations.

SentenceSplitter breaks down documents into manageable chunks by splitting them at natural sentence boundaries.
HuggingFaceEmbedding converts each chunk into numerical embeddings - vector representations that capture the semantic meaning in a way AI can process efficiently.

Example code
from llama_index.core import Document
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.ingestion import IngestionPipeline

# create the pipeline with transformations
pipeline = IngestionPipeline(
    transformations=[
        SentenceSplitter(chunk_overlap=0),
        HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5"),
    ]
)

nodes = await pipeline.arun(documents=[Document.example()])


Now we need to store the the indexes we can do that using ChromaDB



import chromadb
from llama_index.vector_stores.chroma import ChromaVectorStore

db = chromadb.PersistentClient(path="./alfred_chroma_db")
chroma_collection = db.get_or_create_collection("alfred")
vector_store = ChromaVectorStore(chroma_collection=chroma_collection)

pipeline = IngestionPipeline(
    transformations=[
        SentenceSplitter(chunk_size=25, chunk_overlap=0),
        HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5"),
    ],
    vector_store=vector_store,
)


RETRIVING QUERY VIA VECTOR

Before we can query our index, we need to convert it to a query interface. The most common conversion options are:

as_retriever: For basic document retrieval, returning a list of NodeWithScore objects with similarity scores
as_query_engine: For single question-answer interactions, returning a written response
as_chat_engine: For conversational interactions that maintain memory across multiple messages, returning a written response using chat history and indexed context


TOOLS IN LlamaIndex
1. functionTool: allows you to convert any python function into a tool

2. Search Engine: A tool that lets agent use query engines

3. Toolspecs: a set of tools created by the community 

4. Utility Tools:special tools that help handle large amount of data from other tools

